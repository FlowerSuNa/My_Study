# Boosting Algorithms as Gradient Descent

Llew Mason, Jonathan Baxter, Peter Bartlett, Marcus Frean

<br>

### Abstract

* Provide an abstract characteriztion of boosting algorithms as **gradient descent** on cost-functionals in an inner-product function space.

* Prove **convergence** of these functional-gradient-descent algorithms under quite weak conditions.

* Present a new algorithm "**DOOMⅡ**" for performing a gradient descent optimization of such cost functions.

* Experiments on serveral data sets from the UC Irvine repository demonstrate that **DOOMⅡ** generally outperforms **AdaBoost**, especially in **high noise situations**, and that the overfitting behaviour of AdaBoost is predicted by cost functions.
