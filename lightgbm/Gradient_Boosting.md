# Gradient Boosting

<br>

**Gradient Boosting** is a machine learning technique for regression and classification problems, 
which produces a prediction model in the form of an **ensemble** of weak prediction models, typically **decision trees**.

<br>

---

### Boosting

<br>

> Can a set of weak learners create a single strong learner?

**Boosting** is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, 
and a family of machine learning algorithms that convert weak learners to strong ones.

<br>

Most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. 
After a weak learner is added, the data weights are readjusted, known as "**re-weighting**". 
Misclassified input data gain a higher weight and examples that are classified correctly lose weight. 
Thus, future weak learners focus more on the examples that previous weak learners miscalssified.

<br>

---

### Gradient Boosting

<br>



---

### References

<br>

https://en.wikipedia.org/wiki/Boosting_(machine_learning)

https://en.wikipedia.org/wiki/Gradient_boosting
