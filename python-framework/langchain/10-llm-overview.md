# LLM Overview

## LM (Language Model)

- 언어 모델(LM)은 주어진 문맥(Context)이 주어졌을 때, 다음 단어로 적합할 조건부 확률을 계산함
- 다음으로 단어를 선택하는 방법에 따라 단어를 선택하고, 문장을 생성하거나 완성하는 등의 작업을 수행함
    - 결정적 방법 (Deterministic)은 가장 확률이 높은 단어를 선택하는 방법으로, 항상 같은 결과가 나옴 (Greedy Selection)
    - 확률적 방법 (Probabilistic)은 단어의 확률 분포에 따라 단어를 랜덤으로 선택하여, 각 실행마다 다른 결과가 나옴 (Random Sampling)

## LLM 작동 방식

- 프롬프트 입력 (Prompt) → 프롬프트 분석 및 처리 → 모델 처리 (LLM) → 응답 생성 (Completion)
- 우리는 보통 Foundation Model(만들어진 모델)을 사용함
    - Gemma (이미지 처리), Lammma, Qwen (한국어 처리), Exaone 등
- 이전 단어들을 기반으로 다음 단어를 예측하는 방식임 (Auto Regression)
- 한 번에 하나의 단어를 생성하며, 각 단어는 이전 단어들에 의존적임 (순차적 생성)
- 가능한 모든 단어의 확률의 분포를 계산하여 이 분포를 바탕으로 단어를 선택함 (확률적 예측)

## LLM 주요 Parameter

- `max_tokens` : 응답 길이 제어, 생성될 최대 토큰 수 지정
- `temperature` : 출력 다양성 조절 (0~2)
    - 0 : 토큰 확률 분포의 편차가 적음 (정밀도 높음) ➡️ 항상 가장 확률이 높은 토큰 선택 (결정적/일관된 응답)
    - 2 : 토큰 확률 분포의 편차가 커짐 ➡️ 매우 창의적이나 불안정한 응답 가성능 높음
- `top_p` : 상위 확률 토큰 선택 (0~1)
    - 0.9 : 상위 90% 토큰의 확률 분포를 계산하여 sampling 수행 ➡️ 하위 10% 토큰은 버리고 sampling 수행
    - 1 : 모든 토큰의 확률 분포를 계산하여 sampling 수행
- `frequency_penalty` : 이전에 사용된 토큰에 패널티 부여 (-2~2) ➡️ 반복 감소, 다양성 증가
    - 양수 : 단어 반복 감소
    - 음수 : 단어 반복 허용
- `presence penalty` : 새 단어 사용 장려 (-2~2)
    - 양수 : 새로운 주제 도입 장려
    - 음수 : 기존 주제 유지 선호
- `stream` : 스트리밍
    - False : 완성된 응답 반환
    - True : 토큰 단위 실시간 스트리밍
- 

## LLM 서비스 개발 팁

- 모델 양자화 : 모델을 16비트 float에서 4비트로 경량화함 ➡️ 성능이 떨어짐
- LLM은 많이 실험해 보는 것이 중요함
- GPT로 눈높이가 높아져 있음
- 간단한 LLM 작업(분류 문제 등)은 오픈 소스를 써도 웬만하면 잘 작동됨
- MCP, Agent 기능 사용하면 좋음음