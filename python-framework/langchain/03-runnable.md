# LangChainì˜ ì‹¤í–‰ êµ¬ì¡° : Runnableê³¼ LCEL

## Runnable

- LangChainì˜ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ëŠ” `Runnable` ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰ë¨  
- ì‹¤í–‰ ë©”ì„œë“œë¡œëŠ” `.invoke()` (ë‹¨ì¼ ì…ë ¥), `.batch()` (ì—¬ëŸ¬ ì…ë ¥), `.stream()` (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬) ë“±ì„ ì§€ì›í•˜ë©°, ë™ê¸°/ë¹„ë™ê¸° ì²˜ë¦¬ ë°©ì‹ì— ë”°ë¼ ë‹¤ì–‘í•˜ê²Œ í™œìš© ê°€ëŠ¥  
- ëª¨ë“  `Runnable` ì»´í¬ë„ŒíŠ¸ëŠ” `|` ì—°ì‚°ìë¥¼ ì‚¬ìš©í•´ ì—°ê²°í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ì¬ì‚¬ìš©ì„±ê³¼ ì¡°í•©ì„±ì´ ë†’ì€ ì²´ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŒ (LCEL ê¸°ë°˜)

ğŸ’¡ **Tip**: í•˜ë‚˜ì˜ Runnable, í”„ë¡¬í”„íŠ¸, í•¨ìˆ˜ëŠ” ë˜ë„ë¡ í•˜ë‚˜ì˜ ëª…í™•í•œ ê¸°ëŠ¥ë§Œ ìˆ˜í–‰í•˜ë„ë¡ êµ¬ì„±í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
- ë³µì¡í•œ ë¡œì§ì„ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ êµ¬ì„±í•˜ë©´ ê°€ë…ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì´ í¬ê²Œ í–¥ìƒë¨

## LCEL (LangChain Expression Language)

- LCELì€ LangChainì—ì„œ ì»´í¬ë„ŒíŠ¸ë“¤ì„ `|` ì—°ì‚°ìë¡œ ì„ ì–¸ì ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ë°©ì‹ì„
- ì»´í¬ë„ŒíŠ¸ëŠ” ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©°, ì´ì „ ì¶œë ¥ì´ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬ë¨  
- ì •ì˜ëœ ì²´ì¸ì€ í•˜ë‚˜ì˜ `Runnable`ë¡œ ê°„ì£¼ë˜ì–´ ë‹¤ë¥¸ ì²´ì¸ì˜ êµ¬ì„± ìš”ì†Œë¡œ ì¬í™œìš© ê°€ëŠ¥í•¨  
- ë°°ì¹˜ ì‹¤í–‰ ì‹œ ë‚´ë¶€ ìµœì í™”ë¥¼ í†µí•´ ë¦¬ì†ŒìŠ¤ë¥¼ ì ˆì•½í•˜ê³  ì²˜ë¦¬ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ  
- LCELì€ í…ŒìŠ¤íŠ¸, ì‹¤í—˜, ë³µì¡í•œ íë¦„ ì œì–´ ë“± ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ êµ¬ì¡°í™”ëœ ì²´ì¸ì„ ë¹ ë¥´ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆëŠ” íš¨ìœ¨ì ì¸ í‘œí˜„ ë°©ì‹ì„

## RunnableSequence

- ì—¬ëŸ¬ `Runnable`ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ì‹¤í–‰í•¨
- LCELë¡œ ì—°ê²°í•œ ì²´ì¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ `RunnableSequence`ë¡œ ì»´íŒŒì¼ë¨
- ì¼ë°˜ì ìœ¼ë¡œëŠ” LCEL ë¬¸ë²•ì„ í™œìš©í•˜ì—¬ ì„ ì–¸ì ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ë°©ì‹ì„ ì„ í˜¸í•¨

```python
from langchain_core.runnables import RunnableSequence
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

# ì»´í¬ë„ŒíŠ¸ ì •ì˜
prompt = PromptTemplate.from_template("'{text}'ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ëœ ë¬¸ì¥ë§Œì„ ì¶œë ¥í•´ì£¼ì„¸ìš”.")
translator = ChatOpenAI(model="gpt-4.1-mini", temperature=0.3)
output_parser = StrOutputParser()

# RunnableSequence ìƒì„± - í´ë˜ìŠ¤ ìƒì„± ë°©ì‹
translation_chain = RunnableSequence(
    first=prompt,
    middle=[translator],
    last=output_parser
)

# RunnableSequence ìƒì„± - LCEL ë°©ì‹
# translation_chain = prompt | translator | output_parser

result = translation_chain.invoke({"text": "ì•ˆë…•í•˜ì„¸ìš”"})
print(result) 
```

## RunnableParallel

- ì—¬ëŸ¬ `Runnable` ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ êµ¬ì„±í•˜ì—¬ ë³‘ë ¬ì²˜ë¦¬ ê°€ëŠ¥í•¨
- ë™ì¼í•œ ì…ë ¥ê°’ì´ ê° `Runnable`ì— ì „ë‹¬ë˜ë©°, ê²°ê³¼ëŠ” í‚¤-ê°’ í˜•íƒœë¡œ ë°˜í™˜ë¨
- ì£¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬, ë³€í™˜, í¬ë§· ì¡°ì • ë“±ì— í™œìš©ë˜ë©°, ë‹¤ìŒ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ì—ì„œ ìš”êµ¬í•˜ëŠ” ì¶œë ¥ í˜•ì‹ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥í•¨

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from operator import itemgetter

# ì§ˆë¬¸ í…œí”Œë¦¿ ì •ì˜
question_template = """
ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ì…ë ¥ì„ ë¶„ë¥˜í•˜ì„¸ìš”:
- í™”í•™(Chemistry)
- ë¬¼ë¦¬(Physics)
- ìƒë¬¼(Biology)

# ì˜ˆì‹œ:
Q: ì‚¬ëŒì˜ ì—¼ìƒ‰ì²´ëŠ” ëª¨ë‘ ëª‡ê°œê°€ ìˆë‚˜ìš”?
A: ìƒë¬¼(Biology)

Q: {question}
A: """

# ì–¸ì–´ ë¶„ë¥˜ í…œí”Œë¦¿ ì •ì˜
language_template = """
ì…ë ¥ëœ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
- ì˜ì–´
- í•œêµ­ì–´
- ê¸°íƒ€

# ì˜ˆì‹œ:
ì…ë ¥: How many protons are in a carbon atom?
ë‹µë³€: English

ì…ë ¥: {question}
ë‹µë³€: """

# ë‹µë³€ í…œí”Œë¦¿ ì •ì˜
answer_template = """
ë‹¹ì‹ ì€ {topic} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {topic}ì— ê´€í•œ ì§ˆë¬¸ì— {language}ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì§ˆë¬¸: {question}
ë‹µë³€: """

# í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ êµ¬ì„±
answer_prompt = ChatPromptTemplate.from_template(answer_template)
output_parser = StrOutputParser()

# LLM model
llm = ChatOpenAI(
    model="gpt-4.1-mini", 
    temperature=0.3
)

# ë³‘ë ¬ ì²˜ë¦¬ ì²´ì¸ êµ¬ì„±
answer_chain = RunnableParallel({
    "topic": question_chain,            # ì£¼ì œ ë¶„ë¥˜ ì²´ì¸
    "language": language_chain,         # ì–¸ì–´ ê°ì§€ ì²´ì¸
    "question": itemgetter("question")  # ì›ë³¸ ì§ˆë¬¸ ì¶”ì¶œ
}) | answer_prompt | llm | output_parser

# ì²´ì¸ ì‹¤í–‰ ì˜ˆì‹œ
result = answer_chain.invoke({
    "question": "íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
})
print(f"ë‹µë³€: {result}")
```

## RunnableLambda

- ì‚¬ìš©ì ì •ì˜ íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ Runnableë¡œ ë˜í•‘í•˜ì—¬ ì²´ì¸ì— í¬í•¨

## RunnablePassthrough

- ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬í•¨
- `RunnablePassthrough`ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì–´ ì…ë ¥ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ í‚¤ë¡œ ë§¤í•‘í•  ìˆ˜ ìˆìŒ
- íˆ¬ëª…í•œ ë°ì´í„° í”„ë¦„ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…ê³¼ êµ¬ì„±ì´ ìš©ì´í•¨
