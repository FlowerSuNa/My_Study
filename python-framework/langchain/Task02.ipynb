{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2eb7b5f",
   "metadata": {},
   "source": [
    "# [실습 프로젝트] Naive RAG 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdac8f9",
   "metadata": {},
   "source": [
    "- RAG 프롬프트 구성 기준: \n",
    "    - LangChain의 ChatPromptTemplate 클래스 사용\n",
    "    - 변수 처리는 {context}, {question} 형식 사용\n",
    "    - 답변은 한글로 출력되도록 프롬프트 작성\n",
    "    \n",
    "- 아래 템플릿 코드를 기반으로 다음 내용을 참고하여 작성합니다. \n",
    "\n",
    "    1. 프롬프트 구성요소:\n",
    "        - 작업 지침\n",
    "        - 컨텍스트 영역\n",
    "        - 질문 영역\n",
    "        - 답변 형식 가이드\n",
    "\n",
    "    2. 작업 지침:\n",
    "        - 컨텍스트 기반 답변 원칙\n",
    "        - 외부 지식 사용 제한\n",
    "        - 불확실성 처리 방법\n",
    "        - 답변 불가능한 경우의 처리 방법\n",
    "\n",
    "    3. 답변 형식:\n",
    "        - 핵심 답변 섹션\n",
    "        - 근거 제시 섹션\n",
    "        - 추가 설명 섹션 (필요시)\n",
    "\n",
    "    4. 제약사항 반영:\n",
    "        - 답변은 사실에 기반해야 함\n",
    "        - 추측이나 가정을 최소화해야 함\n",
    "        - 명확한 근거 제시가 필요함\n",
    "        - 구조화된 형태로 작성되어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba96d7",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed805db8",
   "metadata": {},
   "source": [
    "## 1. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce288c",
   "metadata": {},
   "source": [
    "1\\) 환경 변수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56875714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a864d09",
   "metadata": {},
   "source": [
    "2\\) 문서 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ef3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader('./data/bart.pdf')\n",
    "pdf_docs = pdf_loader.load()\n",
    "print(f'PDF 문서 개수: {len(pdf_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25f211",
   "metadata": {},
   "source": [
    "3\\) 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce61a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문단이 분리된 경우에도 문장 순서를 올바르게 읽는지 확인\n",
    "pdf_docs[0].page_content[2200:2300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19020c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도표로 시작하는 페이지 데이터 확인\n",
    "pdf_docs[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c7923",
   "metadata": {},
   "source": [
    "## 2. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbc0c0",
   "metadata": {},
   "source": [
    "1\\) Text Split\n",
    "\n",
    "- **Semantic Chunking** 방식으로 텍스트를 분할함<br>\n",
    "    → 임베딩 벡터 간의 **기울기(gradient)** 변화를 기준으로 의미 단위(semantic unit)를 구분함<br>\n",
    "    → 청크 길이에 일관성이 없으며, 문맥에 따라 길이가 유동적으로 결정됨\n",
    "\n",
    "- 길이가 100자 미만인 청크는 이미지 기반 텍스트(OCR 등)로 간주하여 제거함<br>\n",
    "    → 주요 텍스트가 아닌 부가 정보일 가능성이 높기 때문임\n",
    "\n",
    "- 1차 분할된 청크는 길이 편차가 크므로, 문자열 길이 기준으로 재귀적으로 분할하여 최종적으로는 일관된 길이의 청크를 구성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47593268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker \n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"),         # OpenAI 임베딩 사용\n",
    "    breakpoint_threshold_type=\"gradient\",  # 임계값 타입 설정 (gradient, percentile, standard_deviation, interquartile)\n",
    ")\n",
    "chunks = text_splitter.split_documents(pdf_docs)\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_chunks = []\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    content = chunk.page_content\n",
    "    if len(chunk.page_content) < 100:\n",
    "        print(f'{idx}: {content}')\n",
    "    else:\n",
    "        selected_chunks.append(chunk)\n",
    "\n",
    "print(f\"생성된 청크 수: {len(selected_chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in selected_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,                      \n",
    "    chunk_overlap=100,\n",
    "    separators=[\" \\n\", \".\\n\", \". \"],\n",
    ")\n",
    "final_chunks = text_splitter.split_documents(selected_chunks)\n",
    "print(f\"생성된 텍스트 청크 수: {len(final_chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in final_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f5012",
   "metadata": {},
   "source": [
    "2\\) Embedding\n",
    "- 문서 임베딩 도구로 `OpenAIEbeddings` 선택함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e461789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAIEmbeddings 모델 생성\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # 사용할 모델 이름\n",
    "    dimensions=1024\n",
    ")\n",
    "documents = [chunk.page_content for chunk in final_chunks]\n",
    "document_embeddings_openai = embeddings_model.embed_documents(documents)\n",
    "print(f\"임베딩 벡터의 개수: {len(document_embeddings_openai)}\")\n",
    "print(f\"임베딩 벡터의 차원: {len(document_embeddings_openai[0])}\")\n",
    "print(document_embeddings_openai[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ceb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 쿼리와 가장 유사한 문서 찾기 함수\n",
    "def find_most_similar(\n",
    "        query: str, \n",
    "        documents: list,\n",
    "        doc_embeddings: np.ndarray,\n",
    "        embeddings_model\n",
    "    ) -> tuple[str, float]:\n",
    "    \n",
    "    # 쿼리 임베딩: OpenAI 임베딩 사용 \n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "\n",
    "    # 가장 유사한 문서 인덱스 찾기\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "\n",
    "    # 가장 유사한 문서와 유사도 반환: 문서, 유사도\n",
    "    return documents[most_similar_idx], similarities[most_similar_idx]\n",
    "\n",
    "\n",
    "query = \"What is BART architecture?\"\n",
    "most_similar_doc, similarity = find_most_similar(\n",
    "    query, \n",
    "    documents,\n",
    "    document_embeddings_openai, \n",
    "    embeddings_model=embeddings_model\n",
    "    )\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"가장 유사한 문서: {most_similar_doc}\")\n",
    "print(f\"유사도: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af65b2f",
   "metadata": {},
   "source": [
    "## 3. Save Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebf505",
   "metadata": {},
   "source": [
    "- Vectorstore로 `ChromaDB` 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"my_collection2\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734942a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순차적 ID 리스트 생성\n",
    "doc_ids = [f\"DOC_{i}\" for i in range(len(final_chunks))]\n",
    "\n",
    "# 문서를 벡터 저장소에 저장\n",
    "added_doc_ids = chroma_db.add_documents(documents=final_chunks, ids=doc_ids)\n",
    "\n",
    "# 벡터 저장소에 저장된 문서를 확인\n",
    "print(f\"{len(added_doc_ids)}개의 문서가 성공적으로 벡터 저장소에 추가되었습니다.\")\n",
    "print(added_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd1d86",
   "metadata": {},
   "source": [
    "## 4. Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be466ac",
   "metadata": {},
   "source": [
    "- **MMR** 검색으로 상위 3개 문서 검색하는 Retriever 사용함\n",
    "- **Cosine similarity** 를 사용하여 임베딩 품질을 확인함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03949ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_mmr = chroma_db.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "        'k': 3,                 # 검색할 문서의 수\n",
    "        'fetch_k': 8,           # mmr 알고리즘에 전달할 문서의 수 (fetch_k > k)\n",
    "        'lambda_mult': 0.3,     # 다양성을 고려하는 정도 (1은 최소 다양성, 0은 최대 다양성을 의미. 기본값은 0.5)\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 테스트 \n",
    "query = \"What is BART architecture?\"\n",
    "retrieved_docs = chroma_mmr.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    score = cosine_similarity(\n",
    "        [embeddings_model.embed_query(query)], \n",
    "        [embeddings_model.embed_query(doc.page_content)]\n",
    "        )[0][0]\n",
    "    print(f\"-{i}-\\n{doc.page_content[:100]}...{doc.page_content[-100:]} \\n[유사도: {score}]\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb304e9",
   "metadata": {},
   "source": [
    "## 5. Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb87d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 템플릿 (예시)\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "translate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following into English: {query}\"\n",
    ")\n",
    "work_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Please answer following these rules:\n",
    "1. Answer the questions based only on [Context].\n",
    "2. If there is no [Context], answer that you don't know.\n",
    "3. Do not use external knowledge.\n",
    "4. If there is no clear basis in [Context], answer that you don't know.\n",
    "5. You can refer to the previous conversation.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\")\n",
    "output_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following into Korean: {output}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4297d05",
   "metadata": {},
   "source": [
    "## 6. Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d736e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.7\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 문서 포맷팅\n",
    "def format_docs(docs):\n",
    "    \"\"\" 참고 문서 연결 \"\"\"\n",
    "    return \"\\n\\n\".join([f\"{i}: \\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "def format_result(answer):\n",
    "    \"\"\" 최종 응답 처리 \"\"\"\n",
    "    output = answer['output']\n",
    "    context = answer['context']\n",
    "    return f\"{output}\\n\\n[Context]\\n{context}\"\n",
    "\n",
    "# 체인 생성\n",
    "translate_chain = translate_prompt | llm | output_parser\n",
    "rag_chain = chroma_mmr | RunnableLambda(format_docs)\n",
    "output_chain = work_prompt | llm | output_parser | output_prompt | llm | output_parser\n",
    "\n",
    "main_chain = (\n",
    "    translate_chain |\n",
    "    RunnableParallel(\n",
    "        question=RunnablePassthrough(),\n",
    "        context=lambda x: rag_chain.invoke(x),\n",
    "    ) | \n",
    "    RunnableParallel(\n",
    "        context=lambda x: x['context'],\n",
    "        output=output_chain\n",
    "    ) | RunnableLambda(format_result)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"BART의 강점이 모야?\"\n",
    "answer = main_chain.invoke({\"query\": query})\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"답변:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76cdd5",
   "metadata": {},
   "source": [
    "# Chat Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34cc86f",
   "metadata": {},
   "source": [
    "- `Gradio`를 활용하여 Chat Interface를 구현함\n",
    "- 위에서 테스트한 내용을 기능별로 정리하여 구현함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6496d058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6693f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnableMap, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c23cff",
   "metadata": {},
   "source": [
    "`(1) 벡터 저장소 설정`\n",
    "\n",
    "- **text-embedding-3-small** 임베딩 모델을 활용하여 **Chroma** 벡터 저장소를 사용함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb78f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAIEmbeddings 모델 생성\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # 사용할 모델 이름\n",
    "    dimensions=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bc12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"my_collection2\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f0129",
   "metadata": {},
   "source": [
    "`(2) 검색기 정의`\n",
    "\n",
    "- MMR 검색으로 상위 3개 문서를 검색하는 Retriever 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a78f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\" 참고 문서 연결 \"\"\"\n",
    "    return \"\\n\\n\".join([f\"{i}: \\n{doc.page_content}\" for i, doc in enumerate(docs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1fcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_chain(question):\n",
    "    \"\"\" Retriever \"\"\"\n",
    "    chroma_mmr = chroma_db.as_retriever(\n",
    "        search_type='mmr',\n",
    "        search_kwargs={\n",
    "            'k': 3,\n",
    "            'fetch_k': 8,\n",
    "            'lambda_mult': 0.3,\n",
    "        },\n",
    "    )\n",
    "    chain = chroma_mmr | RunnableLambda(format_docs)\n",
    "    return chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd293c",
   "metadata": {},
   "source": [
    "`(3) RAG 프롬프트 구성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f179a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_english_chain(model):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Translate the following into English: {query}\"\n",
    "    )\n",
    "    return prompt | model | StrOutputParser()\n",
    "\n",
    "def to_korean_chain(model):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Translate the following into Korean: {query}\"\n",
    "    )\n",
    "    return prompt | model | StrOutputParser()\n",
    "\n",
    "def get_anwser_chain(model):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"\"\"\n",
    "Please answer following these rules:\n",
    "1. Answer the questions based only on [Context].\n",
    "2. If there is no [Context], answer that you don't know.\n",
    "3. Do not use external knowledge.\n",
    "4. If there is no clear basis in [Context], answer that you don't know.\n",
    "5. You can refer to the previous conversation.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "    )])\n",
    "    return prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6da55",
   "metadata": {},
   "source": [
    "`(4) RAG 체인 구성`\n",
    "\n",
    "- 대화 히스토리는 영문으로 작성된 내용만 저장 및 활용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9b171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94855fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.7\n",
    ")\n",
    "memory_store = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7d8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory(x):\n",
    "    memory_store.append(HumanMessage(content=x[\"question\"]))\n",
    "    memory_store.append(AIMessage(content=x[\"answer\"]))\n",
    "    return f\"{x['korean_answer']}\\n\\n[Context]\\n{x['context']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa59d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streaming_response(message: str, history) -> Iterator[str]:\n",
    "    translate_chain = to_english_chain(model)\n",
    "    korean_chain = to_korean_chain(model)\n",
    "    answer_chain = get_anwser_chain(model)\n",
    "\n",
    "    full_chain = (\n",
    "        translate_chain |\n",
    "        RunnableMap({\n",
    "            \"question\": RunnablePassthrough(),  # English question\n",
    "            \"context\": lambda q: get_context_chain(q),  # get_context는 이미 함수로 있음\n",
    "            \"chat_history\": RunnableLambda(lambda _: memory_store)\n",
    "        }) |\n",
    "        RunnableMap({\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"context\": itemgetter(\"context\"),\n",
    "            \"query\": answer_chain\n",
    "        }) |\n",
    "        RunnableMap({\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"context\": itemgetter(\"context\"),\n",
    "            \"answer\": itemgetter(\"query\"),\n",
    "            \"korean_answer\": korean_chain\n",
    "        }) |\n",
    "        RunnableLambda(update_memory)\n",
    "    )\n",
    "    return full_chain.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ad1078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunhwaryu/Documents/llm-study/prj01/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=get_streaming_response,         # 메시지 처리 함수\n",
    "    title=\"BART에 대해\", # 채팅 인터페이스의 제목\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "# 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ada87ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "# demo 실행 종료\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3169b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is it that makes BART better than BERT?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Based on the [Context], BART generalizes BERT by using a bidirectional encoder like BERT and a left-to-right decoder like GPT, combining these features in a Transformer-based sequence-to-sequence architecture. Additionally, BART's decoder layers perform cross-attention over the encoder's final hidden layer, which BERT does not have. BART also contains roughly 10% more parameters than an equivalently sized BERT model. These architectural differences, along with its training method of corrupting and reconstructing text, contribute to BART achieving better performance on various tasks compared to BERT.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Explain the strengths of BART.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Based on the [Context], the strengths of BART include:\\n\\n- It significantly outperforms previous models leveraging BERT by roughly 6.0 points on all ROUGE metrics, indicating a substantial improvement in text generation quality (Context 0).\\n- BART achieves better performance on dialogue response generation tasks, outperforming previous work on automated metrics when generating responses conditioned on context and persona (Context 0).\\n- Its training approach involves corrupting text with arbitrary noising functions and learning to reconstruct the original text, which helps the model learn robust representations (Context 1).\\n- BART uses a standard Transformer-based neural machine translation architecture that generalizes both BERT’s bidirectional encoder and GPT’s left-to-right decoder, combining their strengths (Context 1).\\n- Despite its simplicity, this architecture enables BART to outperform all existing work on various tasks (Context 2).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Explain the structure of BART.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Based on the [Context], BART uses a standard Transformer-based sequence-to-sequence architecture. It consists of a bidirectional encoder, similar to BERT, and a left-to-right decoder, similar to GPT. Each layer of the decoder performs cross-attention over the final hidden layer of the encoder, as in typical Transformer sequence-to-sequence models. Unlike BERT, BART does not use an additional feed-forward network before word prediction. Overall, BART contains roughly 10% more parameters than an equivalently sized BERT model.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a40d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
