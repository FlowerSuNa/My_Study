{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2eb7b5f",
   "metadata": {},
   "source": [
    "# Naive RAG 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed805db8",
   "metadata": {},
   "source": [
    "## 1. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce288c",
   "metadata": {},
   "source": [
    "1\\) 환경 변수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56875714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba96d7",
   "metadata": {},
   "source": [
    "## 2. Document Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a864d09",
   "metadata": {},
   "source": [
    "1\\) Load Document\n",
    "- **BART** 논문 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa61abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 문서 개수: 10\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 다운로드\n",
    "url = \"https://arxiv.org/pdf/1910.13461.pdf\"\n",
    "with open(\"bart_paper.pdf\", \"wb\") as f:\n",
    "    f.write(requests.get(url).content)\n",
    "\n",
    "# 로컬에서 PDF 로드\n",
    "loader = PyPDFLoader(\"bart_paper.pdf\")\n",
    "docs = loader.load()\n",
    "print(f'PDF 문서 개수: {len(docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce61a4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'al., 2019), the order in which\\nmasked tokens are predicted (Yang et al., 2019), and the\\navailable co'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문단이 분리된 경우에도 문장 순서를 올바르게 읽는지 확인\n",
    "docs[0].page_content[2200:2300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19020c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bidirectional \\nEncoder\\nA  _  C  _  E \\nB       D    \\n(a) BERT: Random tokens are replaced with masks,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 도표로 시작하는 페이지 데이터 확인\n",
    "docs[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbc0c0",
   "metadata": {},
   "source": [
    "2\\) Split Text\n",
    "\n",
    "- **Semantic Chunking** 방식으로 텍스트를 분할함<br>\n",
    "    → 임베딩 벡터 간의 **기울기(gradient)** 변화를 기준으로 의미 단위(semantic unit)를 구분함<br>\n",
    "    → 청크 길이에 일관성이 없으며, 문맥에 따라 길이가 유동적으로 결정됨\n",
    "\n",
    "- 길이가 100자 미만인 청크는 이미지 기반 텍스트(OCR 등)로 간주하여 제거함<br>\n",
    "    → 주요 텍스트가 아닌 부가 정보일 가능성이 높기 때문임\n",
    "\n",
    "- 1차 분할된 청크는 길이 편차가 크므로, 문자열 길이 기준으로 재귀적으로 분할하여 최종적으로는 일관된 길이의 청크를 구성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47593268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 청크 수: 35\n",
      "각 청크의 길이: [380, 1973, 2279, 3429, 108, 511, 7, 8, 34, 4303, 3184, 1086, 133, 770, 108, 3245, 2897, 568, 481, 257, 1287, 723, 2615, 1152, 1516, 1298, 489, 750, 1123, 771, 1099, 662, 127, 156, 1796]\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker \n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    breakpoint_threshold_type=\"gradient\",  # 임계값 타입 설정 (gradient, percentile, standard_deviation, interquartile)\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4a91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6: A B C .\n",
      "7: D E .A .\n",
      "8: C . E . A _ . D _ E . A _C . _ E .\n",
      "생성된 청크 수: 32\n",
      "각 청크의 길이: [380, 1973, 2279, 3429, 108, 511, 4303, 3184, 1086, 133, 770, 108, 3245, 2897, 568, 481, 257, 1287, 723, 2615, 1152, 1516, 1298, 489, 750, 1123, 771, 1099, 662, 127, 156, 1796]\n"
     ]
    }
   ],
   "source": [
    "selected_chunks = []\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    content = chunk.page_content\n",
    "    if len(chunk.page_content) < 100:\n",
    "        print(f'{idx}: {content}')\n",
    "    else:\n",
    "        selected_chunks.append(chunk)\n",
    "\n",
    "print(f\"생성된 청크 수: {len(selected_chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in selected_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f0fc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 텍스트 청크 수: 112\n",
      "각 청크의 길이: [380, 398, 350, 381, 396, 448, 383, 448, 427, 333, 440, 306, 497, 81, 367, 490, 488, 344, 463, 449, 425, 108, 422, 146, 441, 380, 402, 396, 467, 487, 317, 465, 428, 437, 296, 354, 417, 429, 497, 308, 384, 258, 363, 351, 409, 415, 262, 133, 770, 108, 431, 465, 422, 446, 487, 359, 484, 329, 496, 152, 470, 481, 488, 383, 406, 182, 486, 132, 481, 257, 432, 473, 382, 473, 250, 312, 458, 401, 349, 475, 407, 495, 344, 420, 476, 421, 500, 375, 360, 484, 443, 468, 489, 484, 339, 495, 480, 317, 475, 335, 481, 458, 200, 496, 262, 127, 156, 484, 462, 470, 472, 208]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,                      \n",
    "    chunk_overlap=100,\n",
    "    separators=[\" \\n\", \".\\n\", \". \"],\n",
    ")\n",
    "final_chunks = text_splitter.split_documents(selected_chunks)\n",
    "print(f\"생성된 텍스트 청크 수: {len(final_chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in final_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f5012",
   "metadata": {},
   "source": [
    "3\\) Embedding\n",
    "- 문서 임베딩은 `OpenAI`의 **text-embedding-3-small** 모델을 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e461789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터의 개수: 112\n",
      "임베딩 벡터의 차원: 1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1024\n",
    ")\n",
    "documents = [chunk.page_content for chunk in final_chunks]\n",
    "document_embeddings = embeddings_model.embed_documents(documents)\n",
    "print(f\"임베딩 벡터의 개수: {len(document_embeddings)}\")\n",
    "print(f\"임베딩 벡터의 차원: {len(document_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54ceb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: What is BART architecture?\n",
      "가장 유사한 문서: . The architecture is closely related to that used in\n",
      "BERT, with the following differences: (1) each layer of\n",
      "the decoder additionally performs cross-attention over\n",
      "the ﬁnal hidden layer of the encoder (as in the trans-\n",
      "former sequence-to-sequence model); and (2) BERT\n",
      "uses an additional feed-forward network before word-\n",
      "prediction, which BART does not. In total, BART con-\n",
      "tains roughly 10% more parameters than the equiva-\n",
      "lently sized BERT model\n",
      "유사도: 0.6146\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_most_similar(\n",
    "        query: str, \n",
    "        documents: list,\n",
    "        doc_embeddings: np.ndarray,\n",
    "        embeddings_model\n",
    "    ) -> tuple[str, float]:\n",
    "    \"\"\" 쿼리와 가장 유사한 문서를 반환하는 함수 (코사인 유사도 사용) \"\"\"\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    return documents[most_similar_idx], similarities[most_similar_idx]\n",
    "\n",
    "# 유사도 확인\n",
    "query = \"What is BART architecture?\"\n",
    "most_similar_doc, similarity = find_most_similar(\n",
    "    query, \n",
    "    documents,\n",
    "    document_embeddings, \n",
    "    embeddings_model=embeddings_model\n",
    ")\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"가장 유사한 문서: {most_similar_doc}\")\n",
    "print(f\"유사도: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebf505",
   "metadata": {},
   "source": [
    "4\\) Save Vector\n",
    "\n",
    "- 임베딩된 벡터는 벡터스토어로 `ChromaDB` 사용하여 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "227cc9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112개의 문서가 성공적으로 벡터 저장소에 추가되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"my_task02\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "chroma_db.get()\n",
    "\n",
    "# 문서를 벡터 저장소에 저장\n",
    "doc_ids = [f\"DOC_{i}\" for i in range(len(final_chunks))]\n",
    "added_doc_ids = chroma_db.add_documents(documents=final_chunks, ids=doc_ids)\n",
    "print(f\"{len(added_doc_ids)}개의 문서가 성공적으로 벡터 저장소에 추가되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be466ac",
   "metadata": {},
   "source": [
    "5\\) Retriever\n",
    "\n",
    "- **MMR** 기반의 Retriever를 사용하여 문맥 다양성을 고려한 상위 3개 문서 청크를 검색함\n",
    "- 유사도 계산에는 **Cosine Similarity** 를 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b03949ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_mmr = chroma_db.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "        'k': 3,                 # 검색할 문서의 수\n",
    "        'fetch_k': 8,           # mmr 알고리즘에 전달할 문서의 수 (fetch_k > k)\n",
    "        'lambda_mult': 0.3,     # 다양성을 고려하는 정도 (1은 최소 다양성, 0은 최대 다양성을 의미. 기본값은 0.5)\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab2e503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: What is the BART architecture?\n",
      "검색 결과:\n",
      "-1-\n",
      ". The architecture is closely related to that used in\n",
      "BERT, with the following differences: (1) each... not. In total, BART con-\n",
      "tains roughly 10% more parameters than the equiva-\n",
      "lently sized BERT model \n",
      "[유사도: 0.6180143168912577]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-2-\n",
      ". 2 Model\n",
      "BART is a denoising autoencoder that maps a corrupted\n",
      "document to the original document it...gressive decoder. For pre-training,\n",
      "we optimize the negative log likelihood of the original\n",
      "document \n",
      "[유사도: 0.5477388932809816]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-3-\n",
      "In the extreme\n",
      "case, where all information about the source is lost,\n",
      "BART is equivalent to a languag...xtreme\n",
      "case, where all information about the source is lost,\n",
      "BART is equivalent to a language model. \n",
      "[유사도: 0.5143756996743966]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 검색 테스트 \n",
    "query = \"What is the BART architecture?\"\n",
    "retrieved_docs = chroma_mmr.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    score = cosine_similarity(\n",
    "        [embeddings_model.embed_query(query)], \n",
    "        [embeddings_model.embed_query(doc.page_content)]\n",
    "        )[0][0]\n",
    "    print(f\"-{i}-\\n{doc.page_content[:100]}...{doc.page_content[-100:]} \\n[유사도: {score}]\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb304e9",
   "metadata": {},
   "source": [
    "## 3. Prompt Engineering and Chain Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d869dd",
   "metadata": {},
   "source": [
    "1\\) Prompt Engineering\n",
    "\n",
    "- 모든 답변은 제공된 컨텍스트에만 기반하여 작성되도록 함\n",
    "- 외부 지식이나 사전 학습된 일반 상식은 사용하지 않도록 함\n",
    "- 컨텍스트 내 명확한 근거가 없을 경우, **답변할 수 없음**으로 응답하도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fb87d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "translate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following into English: {query}\"\n",
    ")\n",
    "work_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Please answer following these rules:\n",
    "1. Answer the questions based only on [Context].\n",
    "2. If there is no [Context], answer that you don't know.\n",
    "3. Do not use external knowledge.\n",
    "4. If there is no clear basis in [Context], answer that you don't know.\n",
    "5. You can refer to the previous conversation.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\")\n",
    "output_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following into Korean: {output}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd23695e",
   "metadata": {},
   "source": [
    "2\\) Chain Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d736e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.7\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\" 참고 문서 연결 \"\"\"\n",
    "    return \"\\n\\n\".join([f\"{i}: \\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "def format_result(answer):\n",
    "    \"\"\" 최종 응답 처리 \"\"\"\n",
    "    output = answer['output']\n",
    "    context = answer['context']\n",
    "    return f\"{output}\\n\\n[Context]\\n{context}\"\n",
    "\n",
    "# 체인 생성\n",
    "translate_chain = translate_prompt | llm | output_parser\n",
    "rag_chain = chroma_mmr | RunnableLambda(format_docs)\n",
    "output_chain = work_prompt | llm | output_parser | output_prompt | llm | output_parser\n",
    "\n",
    "main_chain = (\n",
    "    translate_chain |\n",
    "    RunnableParallel(\n",
    "        question=RunnablePassthrough(),\n",
    "        context=lambda x: rag_chain.invoke(x),\n",
    "    ) | \n",
    "    RunnableParallel(\n",
    "        context=lambda x: x['context'],\n",
    "        output=output_chain\n",
    "    ) | RunnableLambda(format_result)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18b6989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: BART의 강점이 모야?\n",
      "답변:\n",
      "[Context]를 바탕으로, BART의 강점은 다음과 같습니다:\n",
      "\n",
      "- 모든 ROUGE 지표에서 약 6.0점 가량 이전 BERT 기반 연구를 크게 능가하여 텍스트 생성 작업에서 뛰어난 성능을 보입니다 (Context 0).\n",
      "- 고품질의 샘플 출력을 생성합니다 (Context 0).\n",
      "- CONVAI2 데이터셋의 자동 평가 지표에서 이전 연구들을 능가하며 대화 응답 생성에서 우수한 성과를 보입니다 (Context 0).\n",
      "- BERT와 GPT 사전학습 방식을 모두 일반화한 Transformer 기반 신경망 기계번역 아키텍처를 사용하여 손상 및 재구성(corruption and reconstruction) 접근법으로 학습됩니다 (Context 1).\n",
      "- 판별 작업에서 RoBERTa 및 XLNet과 비슷한 성능을 보여, 단방향 디코더 레이어가 이러한 작업에서 성능 저하를 일으키지 않음을 증명합니다 (Context 2).\n",
      "\n",
      "[Context]\n",
      "0: \n",
      "BART outperforms the\n",
      "best previous work, which leverages BERT, by roughly\n",
      "6.0 points on all ROUGE metrics—representing a sig-\n",
      "niﬁcant advance in performance on this problem. Qual-\n",
      "itatively, sample quality is high (see §6). Dialogue We evaluate dialogue response generation\n",
      "on C ONVAI2 (Dinan et al., 2019), in which agents\n",
      "must generate responses conditioned on both the pre-\n",
      "vious context and a textually-speciﬁed persona. BART\n",
      "outperforms previous work on two automated metrics.\n",
      "\n",
      "1: \n",
      "BART is trained by (1) corrupting text with an\n",
      "arbitrary noising function, and (2) learning a\n",
      "model to reconstruct the original text. It uses\n",
      "a standard Tranformer-based neural machine\n",
      "translation architecture which, despite its sim-\n",
      "plicity, can be seen as generalizing BERT (due\n",
      "to the bidirectional encoder), GPT (with the\n",
      "left-to-right decoder), and many other more re-\n",
      "cent pretraining schemes\n",
      "\n",
      "2: \n",
      ". BART performs comparably to RoBERTa and\n",
      "XLNet, suggesting that BART’s uni-directional decoder layers do not reduce performance on discriminative tasks\n"
     ]
    }
   ],
   "source": [
    "# 체인 테스트\n",
    "query = \"BART의 강점이 모야?\"\n",
    "answer = main_chain.invoke({\"query\": query})\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"답변:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b32197e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: LangChain이 뭐야?\n",
      "답변:\n",
      "모르겠어요.\n",
      "\n",
      "[Context]\n",
      "0: \n",
      ". arXiv preprint arXiv:1907.10529, 2019. Guillaume Lample and Alexis Conneau. Cross-\n",
      "lingual language model pretraining. arXiv preprint\n",
      "arXiv:1901.07291, 2019. Zhenzhong Lan, Mingda Chen, Sebastian Goodman,\n",
      "Kevin Gimpel, Piyush Sharma, and Radu Sori-\n",
      "cut. Albert: A lite bert for self-supervised learn-\n",
      "ing of language representations.\n",
      "\n",
      "1: \n",
      ". Associa-\n",
      "tion for Computational Linguistics. doi: 10.18653/\n",
      "v1/N19-1423. URL https://www.aclweb. org/anthology/N19-1423. Emily Dinan, Varvara Logacheva, Valentin Malykh,\n",
      "Alexander Miller, Kurt Shuster, Jack Urbanek,\n",
      "Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan\n",
      "Lowe, et al. The second conversational in-\n",
      "telligence challenge (convai2).\n",
      "\n",
      "2: \n",
      ". This approach is related to the CLS\n",
      "token in BERT; however we add the additional token\n",
      "to the end so that representation for the token in the\n",
      "decoder can attend to decoder states from the complete\n",
      "input (Figure 3a). 3.2 Token Classiﬁcation Tasks\n",
      "For token classiﬁcation tasks, such as answer endpoint\n",
      "classiﬁcation for SQuAD, we feed the complete doc-\n",
      "ument into the encoder and decoder, and use the top\n",
      "hidden state of the decoder as a representation for each\n",
      "word\n"
     ]
    }
   ],
   "source": [
    "# 체인 테스트\n",
    "query = \"LangChain이 뭐야?\"\n",
    "answer = main_chain.invoke({\"query\": query})\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"답변:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76cdd5",
   "metadata": {},
   "source": [
    "## 4. Chat Interface Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34cc86f",
   "metadata": {},
   "source": [
    "- `Gradio`를 활용하여 Chat Interface를 구현함\n",
    "- 위에서 테스트한 내용을 기능별로 정리하여 구현함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6496d058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6693f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnableMap, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c23cff",
   "metadata": {},
   "source": [
    "1\\) 벡터 저장소 설정\n",
    "\n",
    "- `OpenAI`의 **text-embedding-3-small** 임베딩 모델과 **Chroma** 벡터 저장소를 사용함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb78f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAIEmbeddings 모델 생성\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # 사용할 모델 이름\n",
    "    dimensions=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"my_task02\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f0129",
   "metadata": {},
   "source": [
    "2\\) 검색기 정의\n",
    "\n",
    "- MMR 검색으로 상위 3개 문서를 검색하는 Retriever 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a78f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\" 참고 문서 연결 \"\"\"\n",
    "    return \"\\n\\n\".join([f\"{i}: \\n{doc.page_content}\" for i, doc in enumerate(docs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1fcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_chain(question):\n",
    "    \"\"\" Retriever \"\"\"\n",
    "    chroma_mmr = chroma_db.as_retriever(\n",
    "        search_type='mmr',\n",
    "        search_kwargs={\n",
    "            'k': 3,\n",
    "            'fetch_k': 8,\n",
    "            'lambda_mult': 0.3,\n",
    "        },\n",
    "    )\n",
    "    chain = chroma_mmr | RunnableLambda(format_docs)\n",
    "    return chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd293c",
   "metadata": {},
   "source": [
    "3\\) RAG 프롬프트 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f179a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_english_chain(model):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Translate the following into English: {query}\"\n",
    "    )\n",
    "    return prompt | model | StrOutputParser()\n",
    "\n",
    "def to_korean_chain(model):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Translate the following into Korean: {query}\"\n",
    "    )\n",
    "    return prompt | model | StrOutputParser()\n",
    "\n",
    "def get_anwser_chain(model):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"\"\"\n",
    "Please answer following these rules:\n",
    "1. Answer the questions based only on [Context].\n",
    "2. If there is no [Context], answer that you don't know.\n",
    "3. Do not use external knowledge.\n",
    "4. If there is no clear basis in [Context], answer that you don't know.\n",
    "5. You can refer to the previous conversation.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "    )])\n",
    "    return prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6da55",
   "metadata": {},
   "source": [
    "4\\) RAG 체인 구성\n",
    "\n",
    "- 대화 히스토리는 영문으로 작성된 내용만 저장 및 활용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9b171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94855fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.7\n",
    ")\n",
    "memory_store = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7d8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory(x):\n",
    "    memory_store.append(HumanMessage(content=x[\"question\"]))\n",
    "    memory_store.append(AIMessage(content=x[\"answer\"]))\n",
    "    return f\"{x['korean_answer']}\\n\\n[Context]\\n{x['context']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa59d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streaming_response(message: str, history) -> Iterator[str]:\n",
    "    translate_chain = to_english_chain(model)\n",
    "    korean_chain = to_korean_chain(model)\n",
    "    answer_chain = get_anwser_chain(model)\n",
    "\n",
    "    full_chain = (\n",
    "        translate_chain |\n",
    "        RunnableMap({\n",
    "            \"question\": RunnablePassthrough(),  # English question\n",
    "            \"context\": lambda q: get_context_chain(q),  # get_context는 이미 함수로 있음\n",
    "            \"chat_history\": RunnableLambda(lambda _: memory_store)\n",
    "        }) |\n",
    "        RunnableMap({\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"context\": itemgetter(\"context\"),\n",
    "            \"query\": answer_chain\n",
    "        }) |\n",
    "        RunnableMap({\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"context\": itemgetter(\"context\"),\n",
    "            \"answer\": itemgetter(\"query\"),\n",
    "            \"korean_answer\": korean_chain\n",
    "        }) |\n",
    "        RunnableLambda(update_memory)\n",
    "    )\n",
    "    return full_chain.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e66c9e",
   "metadata": {},
   "source": [
    "5\\) Gradio 스트리밍 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ad1078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunhwaryu/Documents/llm-study/prj01/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=get_streaming_response,         # 메시지 처리 함수\n",
    "    title=\"BART에 대해\", # 채팅 인터페이스의 제목\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "# 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ada87ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "# demo 실행 종료\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3169b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is it that makes BART better than BERT?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Based on the [Context], BART generalizes BERT by using a bidirectional encoder like BERT and a left-to-right decoder like GPT, combining these features in a Transformer-based sequence-to-sequence architecture. Additionally, BART's decoder layers perform cross-attention over the encoder's final hidden layer, which BERT does not have. BART also contains roughly 10% more parameters than an equivalently sized BERT model. These architectural differences, along with its training method of corrupting and reconstructing text, contribute to BART achieving better performance on various tasks compared to BERT.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Explain the strengths of BART.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Based on the [Context], the strengths of BART include:\\n\\n- It significantly outperforms previous models leveraging BERT by roughly 6.0 points on all ROUGE metrics, indicating a substantial improvement in text generation quality (Context 0).\\n- BART achieves better performance on dialogue response generation tasks, outperforming previous work on automated metrics when generating responses conditioned on context and persona (Context 0).\\n- Its training approach involves corrupting text with arbitrary noising functions and learning to reconstruct the original text, which helps the model learn robust representations (Context 1).\\n- BART uses a standard Transformer-based neural machine translation architecture that generalizes both BERT’s bidirectional encoder and GPT’s left-to-right decoder, combining their strengths (Context 1).\\n- Despite its simplicity, this architecture enables BART to outperform all existing work on various tasks (Context 2).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Explain the structure of BART.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Based on the [Context], BART uses a standard Transformer-based sequence-to-sequence architecture. It consists of a bidirectional encoder, similar to BERT, and a left-to-right decoder, similar to GPT. Each layer of the decoder performs cross-attention over the final hidden layer of the encoder, as in typical Transformer sequence-to-sequence models. Unlike BERT, BART does not use an additional feed-forward network before word prediction. Overall, BART contains roughly 10% more parameters than an equivalently sized BERT model.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a40d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
